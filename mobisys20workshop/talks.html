<!DOCTYPE HTML>

<html>
	<head>
		<title>HealthDL: MobiSys2020 Workshop</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<!-- <script>document.write('<script src="http://' + (location.host || 'localhost').split(':')[0] + ':35729/livereload.js?snipver=1"></' + 'script>')</script> -->
		<style type="text/css">
			.alt>strong, b {
				color: rgb(49, 49, 49);
				font-weight: 600;
			}
		</style>
	</head>
	<body class="subpage">

		<!-- Header -->
			<header id="header">
				<div class="inner">
					<a href="#" class="logo">MobiSys2020 Workshop</a>
					<!-- Navigation -->
					<nav id="nav">
						<a href="index.html">Home</a>
						<a href="cfp.html">CFP</a>
						<a href="submissions.html">Submissions</a>
						<a href="program.html">Technical Program</a>
						<a href="keynote.html">Keynote</a>
						<a href="panelist.html">Panelist</a>
					</nav>
					<a href="#navPanel" class="navPanelToggle"><span class="fa fa-bars"></span></a>
				</div>
			</header>

		<!-- Main -->
			<section id="main" class="wrapper" style="padding-top: 12px;">
				<div class="inner">
					<span class="image fit"><img src="images/banner.jpeg" alt="" /></span>
					<!-- <header class="align-center">
						<h2 style="margin: auto;">HealthDL</h2>
						<h3>Deep Learning for Wellbeing Applications Leveraging Mobile Devices and Edge Computing</h3>
					</header> -->

				<!-- Content -->
					<!-- <h2 id="content">Sample Content</h2> -->
					<!-- <p>Praesent ac adipiscing ullamcorper semper ut amet ac risus. Lorem sapien ut odio odio nunc. Ac adipiscing nibh porttitor erat risus justo adipiscing adipiscing amet placerat accumsan. Vis. Faucibus odio magna tempus adipiscing a non. In mi primis arcu ut non accumsan vivamus ac blandit adipiscing adipiscing arcu metus praesent turpis eu ac lacinia nunc ac commodo gravida adipiscing eget accumsan ac nunc adipiscing adipiscing.</p> -->
					<div class="row">
						<div class="12u 12u$(small)">
							<!-- <h3>Sem turpis amet semper</h3> -->
							<h2> Distinguished Researcher Talks </h2> 
							
							<div style="text-align:justify;">

								<h3 id="distalk1">Distinguished Researcher Talk 1</h4>
									<b>Title</b><br>
									Tackling the Challenges of Machine Learning for Mobile Health Systems <br><br>

									<b>Speaker</b><br>
									Guoliang Xing (The Chinese University of Hong Kong) <br><br>

									<b>Abstract</b><br>
									The prominence of mobile devices and recent breakthroughs in machine learning have enabled an emerging class of new mobile health systems which hold the promise of transforming today’s reactive healthcare practice to proactive, individualized care and wellbeing. However, the current mainstream machine learning approaches are largely supervised and must be trained by a large amount of labelled high-quality data. In contrast, due to small form factor and limited sensing capability, today’s off-the-shelf mobile devices can only collect noisy and sporadic samples from users. Moreover, personal health data collected by on-device sensors cannot be uploaded or shared with other devices due to privacy concerns. These challenges have significantly hindered the performance and utility of mobile health systems in real-world settings.    In this talk, I will first discuss new mobile systems that exploit human physiological models and innovative use of sensing modalities to achieve highly robust sensing performance without requiring extensive training. I will describe RunBuddy - the first smartphone-based system for monitoring continuous running rhythm and improving exercise efficiency. To mitigate significant environmental noise during running, RunBuddy integrates novel ambient sensing algorithms and a physiological model called Locomotor Respiratory Coupling (LRC). Based on this result, we also develop BreathCoach, a smart and unobtrusive system using smartwatch and smartphone-based VR for in-home RSA-BT (Respiratory Sinus Arrhythmia biofeedback-based Breathing Training), which is a common cardiorespiratory intervention to diseases such as asthma and an effective exercise to reduce anxiety. BreathCoach continuously monitors key bio-signals such as breathing pattern and inter-beat interval, and then recommends breathing patterns in the form of an intuitive VR game to provide an immersive training experience.     In the second part of this talk, I will discuss a new approach for home activity recognition via federated learning. People spend significant portion of time in home. Characterizing home activities including both individual and family activities is important for studies of health, sociology, and home economics. For instance, risk factors of dementia and childhood obesity are strongly associated with a family’s daily activities, and activity logging is proved a very effective approach to improve the self-awareness and motivate people to modify their behaviors toward a healthy lifestyle. Unfortunately, to date, there has been no unobtrusive and convenient methods to log family activities. I will present several new sensing and federated learning algorithms that enable mobile devices to collaboratively improve the accuracy of home activity sensing without sharing any sensor data, preserving user privacy.
									<br><br>
									<b>Bio</b><br>
									Guoliang Xing is currently a Professor of Information Engineering at The Chinese University of Hong Kong. His research interests include Internet of Things (IoT), Smart Health, Cyber-Physical Systems, security, and wireless networking. He received the B.S. and M.S degrees from Xi’an Jiao Tong University, China, in 1998 and 2001, the D.Sc. degree from Washington University in St. Louis, in 2006. He received two Best Paper Awards and six Best Paper Nominations at first-tier conferences including ICNP, IPSN, and IoTDI. Several mobile health technologies developed in his lab won Best App Awards at the MobiCom conference and were successfully transferred to the industry. He received the NSF CAREER Award in 2010 and the Withrow Distinguished Faculty Award from Michigan State University in 2014. He is a Fellow of IEEE.
									
								<br><br>

								<h3 id="distalk2">Distinguished Researcher Talk 2</h4>
									<b>Title</b><br>
									Olivia Health Analytic Platform <br><br>

									<b>Speaker</b><br>
									Majid Sarrafzadeh (University of California, Los Angeles) <br><br>

									<b>Abstract</b><br>
									Olivia is a health analytical platform that enables exploration between a wide range of features such as regional and socio-economical properties and the severity of COVID-19 outbreak. It uses different visualization techniques to demonstrate the correlations and temporal changes in an efficient manner. It also provides means of crowdsourcing knowledge by allowing individuals to submit their findings to the system to be shared with everyone.  Olivia has a Machine Learning backbone that provides prediction capabilities.
									<br><br>
									<b>Bio</b><br>
									Prof. Sarrafzadeh is currently a Distinguished Professor of Computer Science and Electrical Engineering.  He is a co-founder and co-director of the Center for SMART Health. His recent research interests lie in the area of Embedded Computing and Data Analytics with emphasis on healthcare. Dr. Sarrafzadeh is a Fellow of IEEE. Professor Sarrafzadeh has published more than 550 papers, co-authored 5 books, and is a named inventor on many US patents.  Dr. Sarrafzadeh has collaborated with many industries in the past 30 years. He co-founded two companies around 2000 – they were both acquired around 2004. He has recently co-founded three companies in the area of Technology in Healthcare.
	 
								<br><br>

								<h3 id="distalk3">Distinguished Researcher Talk 3</h4>
									<b>Title</b><br>
									Mobile Health Diagnostics through Audio Signals <br><br>
	
									<b>Speaker</b><br>
									Cecilia Mascolo (University of Cambridge) <br><br>
	
									<b>Abstract</b><br>
									Audio has been used for centuries by doctors as diagnostics technique. In this talk, I plan to reflect on the challenges and opportunities that using audio sourced from mobile and wearables could offer in terms of automated diagnostic tools for disease and progression. I will use examples from my group’s ongoing research on exploring devices collecting audio signals (as well as other more traditional signals) from the human body to understand, track and diagnose health, in particular a case study on COVID-19 (covid-19-sounds.org) we have been working on since the outbreak. I will also talk about the machine learning and data analysis challenges imposed by this sort of data using examples from our collaborations with epidemiologists and clinicians. 									
									
									<br><br>
									<b>Bio</b><br>
									Cecilia Mascolo is the mother of a teenage daughter but also a full professor of Mobile Systems in the Department of Computer Science and Technology at the University of Cambridge. She is co-director of the Centre for Mobile, Wearable System and Augmented Intelligence and the recipient of an ERC Advanced Research Grant. Prior joining Cambridge in 2008, she has been a faculty member in the Department of Computer Science at University College London. She holds a PhD from the University of Bologna. Her research interests are in mobile systems for health, human mobility modeling, sensor systems and networking and mobile data analysis. She has published in a number of top tier conferences and journals in the area and her investigator experience spans projects funded by Research Councils and industry. She has received numerous best paper awards and has served as steering, organizing and programme committee member of mobile, sensor systems, networking, data science conferences and workshops. She has delivered a number of keynote talks at conferences and workshops in the area of mobility, data science, pervasive computing and systems. More details can be found at http://www.cl.cam.ac.uk/users/cm542.
							
								</div>

								<br><br><br>

							<h2> Invited Talks </h2> 
							<div style="text-align:justify;">
								<h3 id="talk1">Invited Talk 1</h4>
									<b>Title</b><br>
									Towards Ultra-Efficient DNN Inference Acceleration on Edge Devices for Wellbeing Applications <br><br>
	
									<b>Speaker</b><br>
									Yanzhi Wang (Northeastern University) <br><br>
	
									<b>Abstract</b><br>
									Various Deep Neural Networks (DNN) have served as the fundamental building blocks of a broad spectrum of machine learning applications due to its superior performance. However, it is not an easy task to apply or deploy deep learning techniques on the rapidly increasing edge devices such as wearable Internet of Things (IoT), smartphones or smart health devices with embedded sensors due to the limited computation and memory resources.  It is desirable to develop efficient systems design and algorithms for the wide deployment of deep learning inferences on edge devices.  To address this problem, we propose a set of hardware-friendly structured model pruning and compiler optimization techniques to accelerate DNN executions on edge devices. The structured model pruning is adopted to satisfy the limited computation and memory resource constraints and enable potential hardware acceleration. In the meantime, the compile optimization is utilized to further implement superior DNN inference acceleration performance on edge devices. With the proposed techniques, we are able to achieve real-time DNN inferences on edge devices as shown in the demo with various DNN applications deployed on mobile devices.  These techniques enable us to explore impactful solutions with deep learning algorithms on cheaper affordable wearable IoTs to help in the well beings of users. Specifically, better personalization of health related solutions can help to care for the health and enhance users' experience with the superior performance of deep learning on smart health devices.						
									<br><br>
									<b>Bio</b><br>
									Yanzhi Wang is currently an assistant professor ta Dept. of ECE at Northeastern University, Boston, MA. He received the B.S. degree from Tsinghua University in 2009, and Ph.D. degree from University of Southern California in 2014. His research interests focus on model compression and platform-specific acceleration of deep learning applications. His research maintains the highest model compression rates on representative DNNs since 09/2018. His work on AQFP superconducting based DNN acceleration is by far the highest energy efficiency among all hardware devices. His recent research achievement, CoCoPIE, can achieve real-time performance on almost all deep learning applications using off-the-shelf mobile devices.
									His work has been published broadly in top conference and journal venues (e.g., ASPLOS, ISCA, MICRO, HPCA, PLDI, ICS, ISSCC, AAAI, ICML, CVPR, ICLR, IJCAI, ECCV, ICDM, ACM MM, DAC, ICCAD, FPGA, LCTES, CCS, VLDB, ICDCS, Infocom, TComputer, TCAD, JSAC, TNNLS, etc.), and has been cited above 6,500 times. He has received four Best Paper Awards, has another ten Best Paper Nominations and four Popular Paper Awards. He has received the U.S. Army Young Investigator Award (YIP), Massachusetts Acorn Innovation Award, and other research awards from Google, MathWorks, etc. He is a senior member of IEEE. Three of his former Ph.D./postdoc students become tenure track faculty at Univ. of Connecticut, Clemson University, and Texas A&M University, Corpse Christi.
							
								<br><br>

								<h3 id="talk2">Invited Talk 2</h4>
									<b>Title</b><br>
									A Deep Learning Framework for Prediction of Readmission Risk After Cancer Surgery from Mobile Data Streams <br><br>
	
									<b>Speaker</b><br>
									Yanzhi Wang (Northeastern University) <br><br>
	
									<b>Abstract</b><br>
									Hospital readmissions cost the US healthcare system billions of dollars annually; they are associated with high mortality rates and listed as the source of stress and suffering for both patients and family members. Accurate and early prediction of readmission risk could enable management of emerging postoperative complications before they escalate into readmissions. However, it is challenging to predict who will be readmitted to the hospital following surgery ahead of time. Traditional approaches to readmission risk stratification rely on static administrative and medical record data and generally classify all surgical oncology patients at high-risk. There are, however, different factors related to day to day behavior and activities that may contribute to or signal increased or decreased risk of readmission after discharge. In our research, we leverage the capability of mobile sensing and deep learning to measure daily readmission risk in cancer patients after discharge. Using data from mobile and Fitbit devices of 49 patients collected over 90 days after discharge from the hospital, we build a probabilistic model in an LSTM structure to infer the risk progression trajectory in each patient from their processed behavioral mobile data. Our results show that using only sensor data, the model can predict the risk progression trajectory aligned with the ground truth data.	
									
									<br><br>
									<b>Bio</b><br>
									Afsaneh Doryab, Ph.D., is a Computer Scientist and an Assistant Professor in the School of Engineering and Applied Science at the University of Virginia. She works on computational modeling of human behavior from data streams collected from ubiquitous and mobile devices in the wild. Dr. Doryab has more than 10 years of research experience in ubiquitous and mobile computing, machine learning, computational modeling of human behavior, context-aware computing, human-computer interaction, and pervasive health. Her research has been supported by the National Science Foundation and the National Institutes of Health.
							</div>




						</div>
					</div>
				<hr class="major" />
				</div>
			</section>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>